# 机器学习

## 一、概论

​		机器学习就是通过数据集（x，y）去找一个函数，例如：我们可以通过分析过去一个youtube channel的流量，找出一个函数，从而对未来该channel的流量进行预测。

## 二、线性拟合

### 1.   基本理论

​		如果有一些离散数据，我们可以对这些数据做线性拟合，定义y=wx+b，显然，==应该找出最合适的w与b，使其对已有数据拥有最好的拟合度==。假设有N份数据，定义y*为拟合值，y为真实值，规定loss=(1/N)\*|y\*-y|（此处为标准差，当然，loss函数可根据实际需求定义），当loss最小时，拟合度越好。

​		然而，一次线性拟合有许多局限性（现实多数情况也不是一次的），如果是折线、曲线怎么办。答案是，==通过多个一次线性的相加可以得到折线，而折线可以拟合曲线==。

​		![image-20220729205507021](https://raw.githubusercontent.com/hoshinojyunn/PicBed/main/image-20220729205507021.png)

​		图中，$\color{red}红色$折线等于一个常量（0号蓝线）+ 数个蓝线（1、2、3号）

![image-20220729205724128](https://raw.githubusercontent.com/hoshinojyunn/PicBed/main/image-20220729205724128.png)

​		对于曲线的拟合，可以通过对折线的拟合完成，其中，数据量越多，精度越高。

### 2.   Sigmoid Function

​		Sigmoid Function是一个表示S型光滑曲线的函数，为的是可以去表示上述的蓝线，而蓝色线不光滑，所以我们也叫它hard sigmoid：

![image-20220729210434922](https://raw.githubusercontent.com/hoshinojyunn/PicBed/main/image-20220729210434922.png)

​		通过调整c、b、w的值，就可以得到不同的S型曲线：

![image-20220729210619791](https://raw.githubusercontent.com/hoshinojyunn/PicBed/main/image-20220729210619791.png)

​		因此，我们可以对折线进行拟合（多个sigmoid$\color{blue}蓝线$相加，再加一个与y轴的交点b）：

​	==y = b + $\sum_1^ic_i$ *sigmoid*($b_i + w_ix_1$)==

### 3.  多项数据拟合 

​		当一条蓝色function有多个数据决定时，比如有三条蓝色function，每条蓝线由三个数据决定：

![image-20220729212655445](https://raw.githubusercontent.com/hoshinojyunn/PicBed/main/image-20220729212655445.png)

​		那么，我们就可以带入折线拟合模型，得到三条蓝线r1、r2、r3，写成矩阵是这样的：

![image-20220729212925822](https://raw.githubusercontent.com/hoshinojyunn/PicBed/main/image-20220729212925822.png)

​		三条蓝线相加、再加上b，就得到了y：

![image-20220729213109842](https://raw.githubusercontent.com/hoshinojyunn/PicBed/main/image-20220729213109842.png)

### 4.   随机梯度下降（SGD）与反向传播

​		**先引入两个概念：**

	1. 损失函数（loss function）：是对于单个样本来说的，表示预测结果与实际结果的偏差。
	1. 代价函数（cost function）：与loss function意义相同，但是对于整个样本总体来说的，一般是loss的加总。

#### （1）反向传播基本原理

​		对于一个问题来说，一个函数可能有上千上万个参数，这些参数用一个**向量**表示，对一个初始网络模型来说，**一个样本的大概训练过程如下**：

1. 设置一个输入
2. 模型取初始向量进行计算
3. 输出一个预测结果
4. loss函数使用预测结果与真实结果计算值
5. 梯度下降找到使loss函数最小的参数向量

​		==所以，我们的关键问题就是找到一个参数向量，使得在一个样本上，loss最小==。

​		==但是有一个问题：神经网络的神经元是固定的，可能这个样本为了得到自己想要的结果，需要把一个神经元数值上调，而另一个样本又想把这个神经元的数值下调。==

**这里简单说明梯度下降的原理：**

* 数据集提供输入x与输出y
* 神经网络通过参数向量计算预测结果y*，然后根据loss函数梯度计算这个参数向量的负梯度，然后：

==新参数向量 = 原参数向量 + 参数向量负梯度*学习速率==

**反向传播原理：**

* 对于==一个样本==想要的参数调整，结果向量的每个数值都对上一层神经元的参数数值做出上调或下调的要求，然后每个神经元再对这些要求取均值，这样的过程依次往前面的层递进，从而对神经网络中的神经元模型进行改变。==细节：反向传播要求对神经元的参数数值进行调整，基于梯度下降计算。==
* 而对于==所有样本==，首先统计每个样本希望对每个参数进行改变的值，然后统计这些值的均值作为这个参数的改变值。但如果对于每一次梯度下降都需要用上每一个训练样本计算的话，那么花的时间就太长了，所以一般我们会这么做：

​		首先把训练样本打乱（shuffle），然后分成很多组minibatch（小包），再计算每一个minibatch的梯度下降的一步（这不是代价函数真正的梯度，毕竟计算真实梯度得用上所有的样本，而非这个子集，所以这样下降的一步不是效率最高的一步），但这样计算也能给你一个大概的方向（记住前面说的：不是最佳的方向），最重要的是，这样能使计算量减轻不少。（==这就是随机梯度下降法==）

​		而如果每一次梯度下降都用上每一个训练样本来算的话，虽然每走一步梯度下降都是下降最快的方向，但是耗时太长（毕竟要对每个样本进行计算统计）。（但是两种方法理论来说到达最低点的时间是一样的？个人猜测）

​		**简单来说：**==用minibatch能给你大概的方向，让你快速动起来下降，而不用则是花大量时间去算你该往哪走，然后才动身；用minibatch能让你在训练时更快地看出训练效果，但不一定效果就在往好的方向变，而不用minibatch效果一定在往好的方向走，但过程缓慢。==

**总结**：

* 反向传播是**单个训练样本想怎样修改权重与偏差（bias）**，**不仅是说每个参数应该变大还是变小，还包括了这些变化的比例是多大才能最快地降低代价（cost）。**
* 在pytorch中，反向传播在计算出loss_result后，可以用loss_result.backward()进行反向传播梯度更新
* 反向传播的目的是为我们更新输出提供一定的依据。

## 三、卷积与池化

### 1.   卷积

#### （1）基本介绍

​		卷积的目的是为了从输入中提取有用的特征。其中的过程是通过卷积核与输入的对应位相乘得到一个特征矩阵，比如：

​		有输入：$$\begin{bmatrix}
0&1&1\\
1&1&0\\
1&0&1\\
\end{bmatrix}$$， 有卷积核：$$\begin{bmatrix}0&1\\1&1\end{bmatrix}$$，那么，stride=1（卷积核每次移动几位），padding=0（输入外边框拓展，一般用0填充）的卷积计算结果为：

$$\begin{bmatrix}
3&2\\
2&1\\
\end{bmatrix}$$，过程：

		1. 卷积核左上角与输入左上角对应，第一位：0*0+1\*1+1\*1+1\*1=3
		1. 卷积核左移一位，左上角与输入(0,1)对应，第二位：0*1+1\*1+1\*1+0\*1=2
		1. 卷积核无法左移，下移到输入左侧开始（移一位），左上角对应(1,0)，第三位：0*1+1\*1+1\*1+0\*1=2
		1. 卷积核左移一位，左上角与输入(1,1)对应，第四位：1*0+0\*1+0\*1+1\*1=1

#### （2）卷积层

​		以pytorch为例，pytorch的神经网络模块中有Conv2d层，即为一个2D卷积层。其中有几个重要参数

args：

* in_channels：输入的通道数，一般彩色图片为三通道RGB，所以这里就是3。
* out_channels：卷积输出的通道数，即为卷积核的数量，==卷积核的维度数应与输入维度数一致==，前面的例子中，输入为二维，则卷积核也为二维，如果输入的是一张图片，而图片tensor为三维，卷积核也为三维。因此，一个卷积核就可以输出一个通道，n个卷积核就可以输出n个通道，所以：==输出通道数\=卷积核数量==

* kernel_size：卷积核的size，上面例子中，卷积核为(H,W)=(2,2)，而卷积核的维度由输入维度决定，这里的size只是决定它在第二维上的大小。
* stride：卷积核在卷积过程中，每次走几步。
* padding：输入的外边框填充，如果有个（3，3）的输入，而padding=2，则输入上下左右各扩充2行变为（5，5），扩充的格子一般0填充。

​		一般在模型构建过程中，我们只知道输入与输出的格式，这时需要用公式判断卷积的stride与padding。

Conv2d的公式：

​	![image-20220802201517082](https://raw.githubusercontent.com/hoshinojyunn/PicBed/main/image-20220802201517082.png)

比如，我们有个图片输入为（3，32，32），需要输出（64，32，32）的格式：

根据公式，这里H<sub>in</sub>为32，H<sub>out</sub>也为32，kernel_size[0]是卷积核的高（假设为5），dilation[0]是空洞卷积的高间距（默认为1），我们可以推出：==stride[0]为在横向移动上的步长，以及padding[0]为上下填充==。

1. 可以根据联立推出
2. 可以根据常理推出

根据常理推测：

* 根据第一条式子，如果stride为1，则padding为2，合理
* 如果stride为2，则padding会有点大，扩充太多显然不合理。

### 2.  池化

#### （1）基本介绍

​		池化的目的是为了从输入中提取关键信息，减少训练时间，通常与卷积层一起，用来减小网络大小。

如果有输入$$\begin{bmatrix}0&1&1\\
1&1&0\\
1&0&1\\
\end{bmatrix}$$， 有池化核大小（2，2），那么，stride=1（池化核每次移动几位），padding=0（输入外边框拓展，一般用0填充）的池化计算结果为：$$\begin{bmatrix}
1&1\\
1&1\\
\end{bmatrix}$$

​		计算规则：与卷积差不多，池化是在池化核内选择输入的最大值，如结果的每一位，因为输入最大的是1，所以每一位都是1。